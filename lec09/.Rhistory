prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- cubic with interactions", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=7, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=7, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- 7th degree polynomial", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
coef(lr_fit_7)
lr_fit_7 = lr_fit
coef(lr_fit_7)
library(glmnet)
bigX_train = as.matrix(df_train[,-ncol(df_train)], nrow=nrow(df_train))
bigX_test = as.matrix(df_test, nrow=nrow(df_test))
lr_fit = cv.glmnet(x=bigX_train, y=g, family="binomial", nfolds=5)
prob = predict(lr_fit$glmnet.fit, bigX_test, type="response", s=lr_fit$lambda.min)
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- lasso", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
library(textir)
library(glmnet)
data(we8there)
head(we8thereRatings)
we8thereCounts[1, we8thereCounts[1, ]!=0]            # words in the first review
we8thereCounts[6000, we8thereCounts[6000, ]!=0]      # words in the review 6000
we8thereCounts[61000, we8thereCounts[6100, ]!=0]      # words in the review 6000
we8thereCounts[6100, we8thereCounts[6100, ]!=0]      # words in the review 6000
y = ifelse(we8thereRatings$Overall>3, 1, 0)
y = as.factor(y)
set.seed(1)
glm_fit = cv.glmnet(x = we8thereCounts, y = y,
family = "binomial",
alpha = 1,                        # lasso - 1, ridge - 0
nfold = 5
)
glm_fit = cv.glmnet(x = we8thereCounts, y = y,
family = "binomial",
alpha = 1,                        # lasso - 1, ridge - 0
nfold = 5
)
plot(glm_fit)
plot(glm_fit$glmnet.fit, xvar = "lambda")
abline(v = log(glm_fit$lambda.1se), lty=2, col="red")
glm.coef = coef(glm_fit$glmnet.fit, s=glm_fit$lambda.1se)
sum(glm.coef!=0)
o = order( glm.coef, decreasing = TRUE )
dimnames(glm.coef)[[1]][o[1:10]]
glm.coef[o[1:10]]
dimnames(glm.coef)[[1]][tail(o,10)]
glm.coef[tail(o,10)]
glm_fit_ridge = cv.glmnet(x = we8thereCounts, y = y,
family = "binomial",
alpha = 0,                        # lasso - 1, ridge - 0
nfold = 5
)
plot(glm_fit_ridge)
plot(glm_fit_ridge$glmnet.fit, xvar = "lambda")
abline(v = log(glm_fit_ridge$lambda.1se), lty=2, col="red")
library(ElemStatLearn)
x <- mixture.example$x
g <- mixture.example$y
xnew <- mixture.example$xnew
px1 <- mixture.example$px1
px2 <- mixture.example$px2
gd <- expand.grid(x=px1, y=px2)
par(mar=rep(2,4))
plot(x, col=ifelse(g==1, "coral", "cornflowerblue"), axes=FALSE)
box()
#dev.copy(pdf, "05_lr_mixture.pdf")
#dev.off()
## logistic regression
df_train = data.frame(x1=x[,1], x2=x[,2], y=g)
df_test = data.frame(x1=xnew[,1], x2=xnew[,2])
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=2, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=2, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- quadratic with interactions", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=3, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=3, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- cubic with interactions", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=7, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=7, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- 7th degree polynomial", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
lr_fit_7 = lr_fit
coef(lr_fit_7)
library(glmnet)
lr_fit = cv.glmnet(x=bigX_train, y=g, family="binomial", nfolds=5)
prob = predict(lr_fit$glmnet.fit, bigX_test, type="response", s=lr_fit$lambda.min)
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- lasso", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
bigX_train = as.matrix(df_train[,-ncol(df_train)], nrow=nrow(df_train))
bigX_test = as.matrix(df_test, nrow=nrow(df_test))
lr_fit = cv.glmnet(x=bigX_train, y=g, family="binomial", nfolds=5)
prob = predict(lr_fit$glmnet.fit, bigX_test, type="response", s=lr_fit$lambda.min)
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- lasso", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
lr_fit_lasso = lr_fit
coef(lr_fit$glmnet.fit, s=lr_fit$lambda.min)
library(textir)
library(glmnet)
data(we8there)
head(we8thereRatings)
we8thereCounts[1, we8thereCounts[1, ]!=0]            # words in the first review
we8thereCounts[6000, we8thereCounts[6000, ]!=0]      # words in the review 6000
we8thereCounts[6100, we8thereCounts[6100, ]!=0]      # words in the review 6000
y = ifelse(we8thereRatings$Overall>3, 1, 0)
y = as.factor(y)
set.seed(1)
glm_fit = cv.glmnet(x = we8thereCounts, y = y,
family = "binomial",
alpha = 1,                        # lasso - 1, ridge - 0
nfold = 5
)
plot(glm_fit)
plot(glm_fit$glmnet.fit, xvar = "lambda")
abline(v = log(glm_fit$lambda.1se), lty=2, col="red")
glm.coef = coef(glm_fit$glmnet.fit, s=glm_fit$lambda.1se)
o = order( glm.coef, decreasing = TRUE )
dimnames(glm.coef)[[1]][o[1:10]]
glm.coef[o[1:10]]
# negative coefficients
dimnames(glm.coef)[[1]][tail(o,10)]
glm.coef[tail(o,10)]
glm_fit_ridge = cv.glmnet(x = we8thereCounts, y = y,
family = "binomial",
alpha = 0,                        # lasso - 1, ridge - 0
nfold = 5
)
plot(glm_fit_ridge)
plot(glm_fit_ridge$glmnet.fit, xvar = "lambda")
abline(v = log(glm_fit_ridge$lambda.1se), lty=2, col="red")
inClass = TRUE
source("mnist.helper.R")
MNIST_DIR = "/home/mkolar/projects/mlRepos/MLClassData/MNISTDigits"
digit.data = load_mnist(MNIST_DIR)
library(h2o)
h2oServer <- h2o.init(ip="localhost", port=54321, max_mem_size="4g", nthreads=2)
train_hex = as.h2o(data.frame(x=digit.data$train$x, y=digit.data$train$y))
test_hex = as.h2o(data.frame(x=digit.data$test$x, y=digit.data$test$y))
predictors = 1:784
response = 785
train_hex[,response] <- as.factor(train_hex[,response])
test_hex[,response] <- as.factor(test_hex[,response])
train_labels = train_hex[,response]
test_labels = test_hex[,response]
trainX = train_hex[,-response]
testX = test_hex[,-response]
nfeatures = 50
auto_encoder = h2o.deeplearning(x=predictors,
training_frame=trainX,
activation="Tanh",
autoencoder=T,
hidden=c(nfeatures),
l1=1e-5,
ignore_const_cols=F,
epochs=1)
train.ae.features = h2o.deepfeatures(auto_encoder, trainX, 1)
test.ae.features = h2o.deepfeatures(auto_encoder, testX, 1)
deep.ae.rf = h2o.loadModel( path = file.path("mnist", "DRF_features.50") )
phat = h2o.predict(deep.ae.rf, test.ae.features)
head(phat)
h2o.confusionMatrix(deep.ae.rf, h2o.cbind(test.ae.features, test_labels))
pca.ae = h2o.loadModel( path = file.path("mnist", "pca.ae") )
pca.ae.train.features = h2o.deepfeatures(pca.ae, trainX, 5)
plot(as.matrix(pca.ae.train.features[1:1000,]),
pch=as.character( digit.data$train$y[1:1000] ),
col=c(1:10)[digit.data$train$y[1:1000]+1])
source("mnist.helper.R")
MNIST_DIR = "/home/mkolar/projects/mlRepos/MLClassData/MNISTDigits"
mnist = load_mnist(MNIST_DIR)
mnist_train <- head(mnist, 60000)
show_digit(mnist, 5)
mnist_train <- head(mnist, 60000)
mnist_test <- tail(mnist, 10000)
mnist_train <- mnist$train$x
mnist_test <- mnist$test$x
mnist_r1000 <- mnist_train[sample(nrow(mnist_train), 1000), ]
pca <- prcomp(mnist_r1000[, 1:784], retx = TRUE)
plot(pca$x[, 1:2], type = 'n')
text(pca$x[, 1:2], labels = mnist_r1000$Label, cex = 0.5,
col = rainbow(length(levels(mnist_r1000$Label)))[mnist_r1000$Label])
pca <- princomp(mnist_r1000[, 1:784], scores = TRUE)
plot(pca$scores[, 1:2], type = 'n')
ind = sample(nrow(mnist_train), 1000)
mnist_r1000 <- mnist_train[ind, ]
mnist_r1000_l <- mnist$train$y[ind, ]
pca <- princomp(mnist_r1000[, 1:784], scores = TRUE)
plot(pca$scores[, 1:2], type = 'n')
text(pca$x[, 1:2], labels = mnist_r1000_l, cex = 0.5,
col = rainbow(length(levels(mnist_r1000_l)))[mnist_r1000_l])
mnist_r1000_l <- mnist$train$y[ind, ]
mnist_r1000_l <- mnist$train$y[ind]
pca <- princomp(mnist_r1000[, 1:784], scores = TRUE)
plot(pca$scores[, 1:2], type = 'n')
text(pca$x[, 1:2], labels = mnist_r1000_l, cex = 0.5,
col = rainbow(length(levels(mnist_r1000_l)))[mnist_r1000_l])
text(pca$scores[, 1:2], labels = mnist_r1000_l, cex = 0.5,
col = rainbow(length(levels(mnist_r1000_l)))[mnist_r1000_l])
show_digit(mnist$train$x[1,])
show_digit(pca$loadings[,1])
show_digit(pca$loadings[,2])
show_digit(pca$loadings[,3])
ind = 1:1000
mnist_r1000 <- mnist_train[ind, ]
mnist_r1000_l <- mnist$train$y[ind]
pca <- princomp(mnist_r1000[, 1:784], scores = TRUE)
# plot the scores of the first two components
plot(pca$scores[, 1:2], type = 'n')
text(pca$scores[, 1:2], labels = mnist_r1000_l, cex = 0.5,
col = rainbow(length(levels(mnist_r1000_l)))[mnist_r1000_l])
mnist_r1000 <- mnist_train[ind, ]
mnist_r1000_l <- mnist$train$y[ind]
library(depmixS4)
library(HMM)
install.packages("HMM")
require(HMM)
nSim = 2000
States = c("Fair", "Unfair")
Symbols = 1:6
transProbs = matrix(c(0.99, 0.01, 0.02, 0.98), c(length(States),
length(States)), byrow = TRUE)
emissionProbs = matrix(c(rep(1/6, 6), c(rep(0.1, 5), 0.5)),
c(length(States), length(Symbols)), byrow = TRUE)
hmm = initHMM(States, Symbols, transProbs = transProbs, emissionProbs = emissionProbs)
sim = simHMM(hmm, nSim)
vit = viterbi(hmm, sim$observation)
psObs = posterior(hmm, sim$observation)
f = forward(hmm, sim$observation)
b = backward(hmm, sim$observation)
i <- f[1, nSim]
j <- f[2, nSim]
probObservations = (i + log(1 + exp(j - i)))
posterior = exp((f + b) - probObservations)
x = list(hmm = hmm, sim = sim, vit = vit, posterior = posterior)
mn = "Fair and unfair die"
xlb = "Throw nr."
ylb = ""
plot(x$sim$observation, ylim = c(-7.5, 6), pch = 3, main = mn,
xlab = xlb, ylab = ylb, bty = "n", yaxt = "n")
axis(2, at = 1:6)
text(0, -1.2, adj = 0, cex = 0.8, col = "black", "True: green = fair die")
text(0, -1.2, adj = 0, cex = 0.8, col = "black", "True: green = fair die")
for (i in 1:nSim) {
if (x$sim$states[i] == "Fair")
rect(i, -1, i + 1, 0, col = "green", border = NA)
else rect(i, -1, i + 1, 0, col = "red", border = NA)
}
text(0, -3.2, adj = 0, cex = 0.8, col = "black", "Most probable path")
for (i in 1:nSim) {
if (x$vit[i] == "Fair")
rect(i, -3, i + 1, -2, col = "green", border = NA)
else rect(i, -3, i + 1, -2, col = "red", border = NA)
}
text(0, -5.2, adj = 0, cex = 0.8, col = "black", "Difference")
differing = !(x$sim$states == x$vit)
for (i in 1:nSim) {
if (differing[i])
rect(i, -5, i + 1, -4, col = rgb(0.3, 0.3, 0.3),
border = NA)
else rect(i, -5, i + 1, -4, col = rgb(0.9, 0.9, 0.9),
border = NA)
}
points(x$posterior[2, ] - 3, type = "l")
text(0, -7.2, adj = 0, cex = 0.8, col = "black", "Difference by posterior-probability")
differing = !(x$sim$states == x$vit)
for (i in 1:nSim) {
if (posterior[1, i] > 0.5) {
if (x$sim$states[i] == "Fair")
rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9),
border = NA)
else rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3),
border = NA)
}
else {
if (x$sim$states[i] == "Unfair")
rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9),
border = NA)
else rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3),
border = NA)
}
}
library(HMM)  #hidden markov model R library
chardat = scan("hmm-characters.txt",what="character")
chardat = scan("hmm-characters.txt",what="character")
setwd("~/GoogleDriveBooth/TeachingMaterials/Machine_Learning/ML2016/lec09/scripts")
chardat = scan("hmm-characters.txt",what="character")
print(table(chardat))
cat("number of characters: ",length(chardat),"\n")
cat("### fit hmm\n")
cds = chardat[1:3000]  #use subset to makes things run fairly fast
states = c("1", "2", "3") #three hiden states
nstates <- length(states)
syms = unique(cds)  # possible observation outcomes
nsyms = length(syms)
set.seed(123)
startP = matrix(runif(nstates), 1, nstates); startP = startP/sum(startP) #probs for initial states
tranP = matrix(runif(nstates * nstates),nstates, nstates) #state transition matrix
tranP = sweep(tranP, 1,rowSums(tranP), FUN = "/")
obP = matrix(runif(nstates * nsyms), nstates, nsyms) #for each state, a distribution on possible observed symbols
obP = sweep(obP, 1, rowSums(obP), FUN = "/")
hmm = initHMM(states, syms, startProbs = startP, transProbs = tranP, emissionProbs = obP) #initialize hmm
cat("*** run BaumWelch (this may take a while, why did the siamese twins move to England?)\n")
BW = baumWelch(hmm, cds) #this may take a little while
cat("so the other one could drive\n")
sts = viterbi(BW$hmm, cds)
cat("### plot fitted hhm\n")
fhmm = BW$hmm
obProb = fhmm$emissionProbs
plot(c(1,nsyms),range(obProb),type="n",xlab="char #",ylab="prob")
title(main="observation (character) probs for different states")
for(i in 1:nstates) text(1:nsyms,obProb[i,],syms,col=i,cex=2)
legend("topleft",legend=paste0("state",1:3),col=1:nstates,pch=rep(16,nstates))
dev.copy2pdf(file="obs-probs.pdf",height=6,width=12)
tProb = fhmm$transProbs
plot(c(1,nstates),range(tProb),type="n",xlab="state",ylab="state Prob")
for(i in 1:nstates) lines(1:nstates,tProb[i,],col=i,lwd=2)
legend("topleft",legend=paste0("state",1:3),col=1:nstates,lwd=rep(2,nstates))
title(main="transition probabilities")
plot(as.numeric(sts[1:50]),xlab="observation #",ylab="state",type="b",pch=16,col="blue")
install.packages("depmixS4")
install.packages("TTR")
install.packages("xts")
library(xts)
library(TTR)
library(depmixS4)
Sys.setenv(tz = "UTC")
sp500 <- getYahooData("^GSPC", start = 19500101, end = 20120909, freq = "daily")
ep <- endpoints(sp500, on = "months", k = 1)
sp500 <- sp500[ep[2:(length(ep)-1)]]
sp500$logret <- log(sp500$Close) - lag(log(sp500$Close))
sp500 <- na.exclude(sp500)
plot(sp500$logret, main = "S&P 500 log Returns")
mod <- depmix(logret ~ 1, family = gaussian(), nstates = 4, data = sp500)
set.seed(1)
fm2 <- fit(mod, verbose = FALSE)
summary(fm2, which = "prior")
summary(fm2, which = "transition")
summary(fm2, which = "response")
tsp500 <- as.ts(sp500)
pbear <- as.ts(posterior(fm2)[, 2])
tsp(pbear) <- tsp(tsp500)
plot(cbind(tsp500[, 6], pbear),
main = "Posterior Probability of State=1 (Volatile, Bear Market)")
map.bear <- as.ts(posterior(fm2)[, 1] == 1)
tsp(map.bear) <- tsp(tsp500)
plot(cbind(tsp500[, 6], map.bear),
main = "Maximum A Posteriori (MAP) State Sequence")
tsp500 <- as.ts(sp500)
pbear <- as.ts(posterior(fm2)[, 2])
tsp(pbear) <- tsp(tsp500)
plot(cbind(tsp500[, 6], pbear),
main = "Posterior Probability of State=1 (Volatile, Bear Market)")
library(depmixS4) #the HMM library
library(quantmod) #a great library for technical analysis and working with time series
install.packages("quantmod")
library(quantmod) #a great library for technical analysis and working with time series
library(ggplot2)
library(gridExtra)
EURUSD1d = read.csv("EURUSD1d.csv")
head(EURUSD1d)
EURUSD1d = read.csv("EURUSD1d.csv")
head(EURUSD1d)
Date<-as.character(EURUSD1d[,1])
DateTS<- as.POSIXlt(Date, format = "%Y.%m.%d %H:%M:%S") #create date and time objects
TSData<-data.frame(EURUSD1d[,2:5],row.names=DateTS)
TSData<-as.xts(TSData) #build our time series data set
ATRindicator<-ATR(TSData, n=14) #calculate the indicator
ATR<-ATRindicator[,2] #grab just the ATR
LogReturns <- log(EURUSD1d$Close) - log(EURUSD1d$Open) #calculate the logarithmic returns
ModelData<-data.frame(LogReturns,ATR) #create the data frame for our HMM model
ModelData<-ModelData[-c(1:14),] #remove the data where the indicators are being calculated
colnames(ModelData)<-c("LogReturns","ATR") #name our columns
set.seed(12)
HMM<-depmix(list(LogReturns~1,ATR~1),data=ModelData,nstates=3,family=list(gaussian(),gaussian())) #We’re setting the LogReturns and ATR as our response variables, using the data frame we just built, want to set 3 different regimes, and setting the response distributions to be gaussian.
HMMfit<-fit(HMM, verbose = FALSE) #fit our model to the data set
print(HMMfit) #we can compare the log Likelihood as well as the AIC and BIC values to help choose our model
summary(HMMfit)
HMMpost<-posterior(HMMfit) #find the posterior odds for each state over our data set
head(HMMpost) #we can see that we now have the probability for each state for everyday as well as the highest probability class.
DFIndicators <- data.frame(DateTS, LogReturns, ATR);
DFIndicatorsClean <- DFIndicators[-c(1:14), ]
Plot1Data<-data.frame(DFIndicatorsClean, HMMpost$state)
LogReturnsPlot = ggplot(Plot1Data,aes(x=Plot1Data[,1],y=Plot1Data[,2]))+geom_line(color="darkblue")+labs(title="",y="Log Returns",x="Date")
ATRPlot = ggplot(Plot1Data,aes(x=Plot1Data[,1],y=Plot1Data[,3]))+geom_line(color="darkblue")+labs(title="",y="ATR(14)",x="Date")
RegimePlot = ggplot(Plot1Data,aes(x=Plot1Data[,1],y=Plot1Data[,4]))+geom_line(color="darkblue")+labs(title="",y="Regime",x="Date")
grid.arrange(LogReturnsPlot,ATRPlot,RegimePlot,ncol=1,nrow=3)
RegimePlotData<-data.frame(Plot1Data$DateTS,HMMpost)
Regime1Plot = ggplot(RegimePlotData,aes(x=RegimePlotData[,1],y=RegimePlotData[,3]))+geom_line(color="purple")+labs(title="Regime 1",y="Probability",x="Date")
Regime2Plot = ggplot(RegimePlotData,aes(x=RegimePlotData[,1],y=RegimePlotData[,4]))+geom_line(color="purple")+labs(title="Regime 2",y="Probability",x="Date")
Regime3Plot = ggplot(RegimePlotData,aes(x=RegimePlotData[,1],y=RegimePlotData[,5]))+geom_line(color="purple")+labs(title="Regime 3",y="Probability",x="Date")
grid.arrange(Regime1Plot,Regime2Plot,Regime3Plot,ncol=1,nrow=3)
rm(list = ls())
library(depmixS4)
library(TTR)
library(xts)
## Bull and Bear Markets ##
# Load S&P 500 returns
Sys.setenv(tz = "UTC")
sp500 <- getYahooData("^GSPC", start = 19500101, end = 20120909, freq = "daily")
# Preprocessing
ep <- endpoints(sp500, on = "months", k = 1)
sp500 <- sp500[ep[2:(length(ep)-1)]]
sp500$logret <- log(sp500$Close) - lag(log(sp500$Close))
sp500 <- na.exclude(sp500)
# Plot the S&P 500 returns
plot(sp500$logret, main = "S&P 500 log Returns")
# Regime switching model
mod <- depmix(logret ~ 1, family = gaussian(), nstates = 4, data = sp500)
set.seed(1)
fm2 <- fit(mod, verbose = FALSE)
# Initial probabilities
summary(fm2, which = "prior")
# Transition probabilities
summary(fm2, which = "transition")
# Reponse/emission function
summary(fm2, which = "response")
library(depmixS4)
install.packages("depmixS4")
install.packages("TTR")
install.packages("xts")
library(depmixS4)
library(TTR)
library(xts)
## Bull and Bear Markets ##
# Load S&P 500 returns
Sys.setenv(tz = "UTC")
sp500 <- getYahooData("^GSPC", start = 19500101, end = 20120909, freq = "daily")
# Preprocessing
ep <- endpoints(sp500, on = "months", k = 1)
sp500 <- sp500[ep[2:(length(ep)-1)]]
sp500$logret <- log(sp500$Close) - lag(log(sp500$Close))
sp500 <- na.exclude(sp500)
plot(sp500$logret, main = "S&P 500 log Returns")
mod <- depmix(logret ~ 1, family = gaussian(), nstates = 4, data = sp500)
set.seed(1)
fm2 <- fit(mod, verbose = FALSE)
summary(fm2, which = "prior")
summary(fm2, which = "transition")
summary(fm2, which = "response")
tsp500 <- as.ts(sp500)
pbear <- as.ts(posterior(fm2)[, 2])
tsp(pbear) <- tsp(tsp500)
plot(cbind(tsp500[, 6], pbear),
main = "Posterior Probability of State=1 (Volatile, Bear Market)")
map.bear <- as.ts(posterior(fm2)[, 1] == 1)
tsp(map.bear) <- tsp(tsp500)
plot(cbind(tsp500[, 6], map.bear),
main = "Maximum A Posteriori (MAP) State Sequence")
library(depmixS4) #the HMM library
library(quantmod) #a great library for technical analysis and working with time series
library(ggplot2)
library(gridExtra)
install.packages("quantmod")
install.packages("ggplot2")
install.packages("gridExtra")
